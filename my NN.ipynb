{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "### Nothing loads temporarily. Later it should be overload.\n",
    "###\n",
    "\n",
    "def load_parameters():\n",
    "    X = np.random.randn(64*64, 200)\n",
    "    Y = np.random.randn(2, 200)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(X, hidden_layers, Y, sigma):\n",
    "    W, b = {}, {}\n",
    "    n_L = len(hidden_layers)      # number of layers\n",
    "\n",
    "    # Init (n_H, n_W) matrix for each layer exclude output\n",
    "    n_W = X.shape[0]              # width\n",
    "    for i in range(n_L):\n",
    "        n_H = hidden_layers[i]    # height\n",
    "        l = str(i + 1)            # index of layer\n",
    "        W[\"W\" + l] = np.random.randn(n_H, n_W) * sigma\n",
    "        b[\"b\" + l] = np.zeros((n_H, 1))\n",
    "        n_W = n_H                 # the next layer contains the same width as the height of the previous layer\n",
    "\n",
    "    # Init (n_H, n_W) matrix for output\n",
    "    l = str(n_L + 1)\n",
    "    n_H = Y.shape[0]              # final height, number of outputs\n",
    "    W[\"W\" + l] = np.random.randn(n_H, n_W) * sigma\n",
    "    b[\"b\" + l] = np.zeros((n_H, 1))\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = Z * (Z > 0)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_step_propagation(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_calculation(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    return -np.sum((Y * np.log(A) + (1 - Y) * np.log(1 - A))) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W, b, Y):\n",
    "    A = {}                        # A[i] = g(W[i] * X[i-1] + b[i]), it's need for backward propagation too \n",
    "    n_L = len(W)                  # number of layers\n",
    "\n",
    "    # each layer uses ReLU for activation\n",
    "    A_prev = X\n",
    "    for l in range(1, n_L):\n",
    "        A[\"A\" + str(l)] = single_step_propagation(A_prev, W[\"W\" + str(l)], b[\"b\" + str(l)], \"relu\")\n",
    "        A_prev = A[\"A\" + str(l)]\n",
    "\n",
    "    # last layer uses Sigmoid for activation\n",
    "    A[\"A\" + str(n_L)] = single_step_propagation(A_prev, W[\"W\" + str(n_L)], b[\"b\" + str(n_L)], \"sigmoid\")\n",
    "\n",
    "    cost = cost_calculation(A[\"A\" + str(n_L)], Y)\n",
    "\n",
    "    return cost, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_sigmoid(W, b, A, dW, db):\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_relu(W, b, A, dW, db):\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propogation(W, b, A):\n",
    "    dW, db = {}, {}\n",
    "    dW, db = backward_sigmoid(W, b, A, dW, db)\n",
    "    dW, db = backward_relu(W, b, A, dW, db)\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, hidden_layers, learning_rate=0.01, sigma=0.01, iteration=1000):\n",
    "    W, b = initialize_parameters(X, hidden_layers, Y, sigma)\n",
    "    for i in range(iteration):\n",
    "        cost, A = forward_propagation(X, W, b, Y)\n",
    "        dW, db = backward_propogation(W, b, A)\n",
    "        if i % 100 == 0:\n",
    "            print(\"cost =\", cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
